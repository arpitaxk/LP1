{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24073c-cc7b-4bf2-a622-ae4e10e89aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes age, salary, purchased\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "data = pd.read_csv('NaiveBayes.csv')\n",
    "data.head()\n",
    "X = data[['Age', 'Salary']]\n",
    "y = data['Purchased']\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "nb_classifier = GaussianNB()\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy :{accuracy*100:.2f}\")\n",
    "cm = confusion_matrix( y_test, y_pred)\n",
    "print(\"Confusion Matrix\\n\", cm)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report\\n\", cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b98c42-1d53-4be6-8c65-5cfaa72f6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes prima indians diabetes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "column_names = [\n",
    "    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', \n",
    "    'BMI', 'DiabetesPedigree', 'Age', 'Outcome']\n",
    "df = pd.read_csv('pima-indians-diabetes.csv', names = column_names , header= None, skiprows = 9)\n",
    "#removing the unwanted values by changing to numeric values\n",
    "df = df[pd.to_numeric(df['Pregnancies'], errors='coerce').notnull()]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "# Features (X) - All columns except the last one (Outcome)\n",
    "X = df.iloc[:, :-1]\n",
    "# Target (y) - The last column (Outcome)\n",
    "y = df.iloc[:, -1]\n",
    "# Check the first few rows of features and target\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3ae1e-75c5-4d51-bb1e-4c7d4c6426c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#social media naive bayes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('data/Social_Network_Ads.csv')\n",
    "print(df.head())\n",
    "df = df.drop(columns=['User ID'])\n",
    "df['Gender'] = LabelEncoder().fit_transform(df['Gender'])\n",
    "X = df[['Gender', 'Age', 'EstimatedSalary']]  # Features (Gender, Age, EstimatedSalary)\n",
    "y = df['Purchased']  # Target variable (Purchased)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea6f71-4503-42b8-80c3-8145d3c8fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diabetes regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "df = pd.read_csv('data/diabetes.csv')\n",
    "df = df.dropna()\n",
    "X = df[['Glucose']]  # Use 'Glucose' as the predictor feature\n",
    "y = df['Outcome']    # 'Outcome' is the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Coefficient (Slope):\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "rss = np.sum((y_test - y_pred) ** 2)\n",
    "print(\"Residual Sum of Squares (RSS):\", rss)\n",
    "r2_score = model.score(X_test, y_test)\n",
    "print(\"Coefficient of Determination (R²):\", r2_score)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "accuracy = r2_score * 100  # In percentage terms\n",
    "print(\"Accuracy (R² score in percentage):\", accuracy)\n",
    "# Plot the regression line\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression Line')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('Outcome')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1420cedb-f188-47cb-a81e-fa97bc06398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sat vs gpa regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "df = pd.read_csv('data/1.01. Simple linear regression.csv')\n",
    "X = df[['SAT']]  # Independent variable (SAT score)\n",
    "y = df['GPA']   # Dependent variable (GPA)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Coefficient (Slope):\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "# Calculate Residual Sum of Squares (RSS)\n",
    "rss = np.sum((y_test - y_pred) ** 2)\n",
    "print(\"Residual Sum of Squares (RSS):\", rss)\n",
    "# Coefficient of Determination (R²)\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(\"Coefficient of Determination (R²):\", r2)\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "# Plot the regression line\n",
    "plt.scatter(X, y, color='blue', label='Data Points')  # Scatter plot of the data\n",
    "plt.plot(X, model.predict(X), color='red', label='Regression Line')  # Plot the regression line\n",
    "plt.title('Linear Regression: SAT vs GPA')\n",
    "plt.xlabel('SAT Score')\n",
    "plt.ylabel('GPA')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc627e39-a18b-42ea-9ad1-a384e9c18ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#advertising regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "df = pd.read_csv('data/advertising.csv')\n",
    "y = df['Sales']\n",
    "def perform_regression(feature_name):\n",
    "    # Define the independent variable (X)\n",
    "    X = df[[feature_name]]  # We are using only one feature (TV, Radio, or Newspaper) \n",
    "    # Split the data into training and testing sets (80-20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Initialize the Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Display the coefficients\n",
    "    print(f\"\\n{feature_name} Model Coefficients:\")\n",
    "    print(\"Coefficient (Slope):\", model.coef_)\n",
    "    print(\"Intercept:\", model.intercept_)\n",
    "    # Model Evaluation Metrics\n",
    "    print(f\"\\nModel Evaluation for {feature_name}:\")\n",
    "    # Coefficient of Determination (R²)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"Coefficient of Determination (R²):\", r2)\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    # Plot the regression line\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(X, y, color='blue', label='Data Points')  # Scatter plot of the data\n",
    "    plt.plot(X, model.predict(X), color='red', label=f'Regression Line ({feature_name})')  # Plot the regression line\n",
    "    plt.title(f'Linear Regression: {feature_name} vs Sales')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "perform_regression('TV')\n",
    "perform_regression('Radio')\n",
    "perform_regression('Newspaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38937826-9c36-4856-9083-723c80ec178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cities hierarchical clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "df = pd.read_csv('data/cities_r2.csv')\n",
    "# We are interested in the 'effective_literacy_rate_total' column for clustering\n",
    "df_literacy = df[['effective_literacy_rate_total']]\n",
    "print(df_literacy.isnull().sum())\n",
    "scaler = StandardScaler()\n",
    "df_literacy_scaled = scaler.fit_transform(df_literacy)\n",
    "Z = linkage(df_literacy_scaled, method='ward')\n",
    "# Step 4: Plot the Dendrogram to decide the number of clusters\n",
    "plt.figure(figsize=(5, 5))\n",
    "dendrogram(Z)\n",
    "plt.title('Dendrogram for Hierarchical Clustering')\n",
    "plt.xlabel('Cities')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()\n",
    "# Fit the model with 3 clusters\n",
    "hc = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')\n",
    "# Add the cluster labels to the original dataframe\n",
    "df['Cluster'] = hc.fit_predict(df_literacy_scaled)\n",
    "# Step 6: Visualizing the clusters\n",
    "# Here we plot the cities based on their effective literacy rate and color them by clusters\n",
    "sns.scatterplot(x=df['effective_literacy_rate_total'], \n",
    "                y=np.zeros_like(df['effective_literacy_rate_total']), \n",
    "                hue=df['Cluster'], \n",
    "                palette='viridis', \n",
    "                s=100, alpha=0.7)\n",
    "plt.title('Hierarchical Clustering - Literacy Rate')\n",
    "plt.xlabel('Effective Literacy Rate Total')\n",
    "plt.ylabel('Cluster')\n",
    "plt.show()\n",
    "print(df[['name_of_city', 'effective_literacy_rate_total', 'Cluster']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a760d-0cf6-419b-a93c-b295b01915a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hitters hierarchical clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('data/hitters.csv')\n",
    "df_cruns = df[['CRuns']]\n",
    "scaler = StandardScaler()\n",
    "df_cruns_scaled = scaler.fit_transform(df_cruns)\n",
    "Z = linkage(df_cruns_scaled, method='ward')\n",
    "#dendogram\n",
    "plt.figure(figsize=(4,4))\n",
    "dendrogram(Z)\n",
    "plt.title('Dendrogram for Hierarchical Clustering (CRuns)')\n",
    "plt.xlabel('Players')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()\n",
    "hc = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')\n",
    "# Assign cluster labels to the dataframe\n",
    "df['Cluster'] = hc.fit_predict(df_cruns_scaled)\n",
    "# Plot the 'CRuns' values and color them based on their cluster\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.scatterplot(x=df['CRuns'], \n",
    "                y=np.zeros_like(df['CRuns']), \n",
    "                hue=df['Cluster'], \n",
    "                palette='viridis', \n",
    "                s=50, alpha=0.7)\n",
    "plt.title('Hierarchical Clustering - CRuns')\n",
    "plt.xlabel('CRuns (Clustered)')\n",
    "plt.ylabel('Cluster')\n",
    "plt.show()\n",
    "#cluster count\n",
    "print(df[['AtBat', 'Hits', 'HmRun', 'CRuns', 'Cluster']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d950664-8272-4093-877e-f596c5e922dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#startups hierarchical\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('data/50_Startups.csv')\n",
    "abel_encoder = LabelEncoder()\n",
    "df['STATE'] = label_encoder.fit_transform(df['STATE'])\n",
    "# Select the numerical columns for scaling\n",
    "numerical_columns = ['RND', 'ADMIN', 'MKT', 'PROFIT']\n",
    "scaler = StandardScaler()\n",
    "# Scale the data\n",
    "df_scaled = df[numerical_columns]\n",
    "df_scaled = scaler.fit_transform(df_scaled)\n",
    "Z = linkage(df_scaled, method='ward')\n",
    "# Step 4: Plot the Dendrogram to decide the number of clusters\n",
    "plt.figure(figsize=(4,4))\n",
    "dendrogram(Z)\n",
    "plt.title('Dendrogram for Hierarchical Clustering')\n",
    "plt.xlabel('Startups')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()\n",
    "clusters = fcluster(Z, t=3, criterion='maxclust')\n",
    "df['Cluster'] = clusters\n",
    "print(df[['RND', 'ADMIN', 'MKT', 'PROFIT', 'Cluster']].head())\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.scatterplot(x=df['RND'], y=df['PROFIT'], hue=df['Cluster'], palette='viridis', s=50, alpha=0.7)\n",
    "plt.title('Hierarchical Clustering - RND vs PROFIT')\n",
    "plt.xlabel('RND')\n",
    "plt.ylabel('PROFIT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18106c9-964f-4d0c-bd06-169c1ec69889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descision tree loan eligibility\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "df = pd.read_csv('data/madfhantr.csv')\n",
    "print(df.isnull().sum())\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n",
    "df.dropna(subset=['Loan_Status'], inplace=True)\n",
    "df['Dependents'] = df['Dependents'].replace('3+', 3)\n",
    "# Handle missing values in 'Dependents' column\n",
    "df['Dependents'] = df['Dependents'].fillna(0)  # Replace NaN with 0 or another reasonable value\n",
    "# Convert 'Dependents' column to integer\n",
    "df['Dependents'] = df['Dependents'].astype(int)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].median(), inplace=True)\n",
    "df.dropna(subset=['Loan_Amount_Term'], inplace=True)\n",
    "df['Credit_History'].fillna(df['Credit_History'].median(), inplace=True)\n",
    "df.dropna(subset=['Credit_History'], inplace=True)\n",
    "for column in ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status']:\n",
    "    df[column] = LabelEncoder().fit_transform(df[column])\n",
    "X = df[['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome',\n",
    "        'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area']]\n",
    "y = df['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train_scaled, y_train)\n",
    "y_pred = dt_classifier.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy Score of the Decision Tree Classifier:\", accuracy)\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "plt.figure(figsize=(9,9))\n",
    "tree.plot_tree(dt_classifier, filled=True, feature_names=X.columns, class_names=['N', 'Y'], rounded=True, proportion=False, precision=2)\n",
    "plt.title(\"Decision Tree - Loan Eligibility\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9beef1c-3eb8-47c5-8595-6cc26736b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descision tree diabetes\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "column_names = [\n",
    "    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', \n",
    "    'BMI', 'DiabetesPedigree', 'Age', 'Outcome']\n",
    "df = pd.read_csv('data/pima-indians-diabetes.csv', names = column_names , header= None, skiprows = 9)\n",
    "#removing the unwanted values by changing to numeric values\n",
    "df = df[pd.to_numeric(df['Pregnancies'], errors='coerce').notnull()]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "X = df.drop('Outcome', axis=1)  # Features (all columns except 'Outcome')\n",
    "y = df['Outcome']  # Target variable (Outcome)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(dt_classifier, filled=True, feature_names=X.columns, class_names=['No Diabetes', 'Diabetes'], rounded=True)\n",
    "plt.title(\"Decision Tree - Diabetes Prediction\")\n",
    "plt.show()\n",
    "#random sample test\n",
    "random_sample = X_test.sample(1)  # Pick a random sample from the test set\n",
    "random_sample_prediction = dt_classifier.predict(random_sample)  # Predict the outcome for this sample\n",
    "# Print the random sample and its prediction\n",
    "print(\"\\nRandom Sample Prediction:\")\n",
    "print(random_sample)\n",
    "print(f\"Prediction (1 = Diabetes, 0 = No Diabetes): {random_sample_prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f801b1-c87e-4233-ad85-6851f7e88081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means cities r2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('data/cities_r2.csv')\n",
    "print(df.isnull().sum())  # Check for missing values\n",
    "# For simplicity, let's drop rows with missing values. Alternatively, we can fill with mean or mode\n",
    "df = df.dropna()\n",
    "abel_encoder = LabelEncoder()\n",
    "# Encode categorical columns\n",
    "df['state_name'] = label_encoder.fit_transform(df['state_name'])\n",
    "df['location'] = label_encoder.fit_transform(df['location'])\n",
    "X = df[['total_graduates']]  # We're focusing on total_graduates for clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.scatterplot(x=df.index, y='total_graduates', hue='Cluster', palette='viridis', data=df, s=100)\n",
    "plt.title('K-Means Clustering based on Total Graduates')\n",
    "plt.xlabel('City Index')\n",
    "plt.ylabel('Total Graduates')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "print(df[['name_of_city', 'total_graduates', 'Cluster']].sort_values(by='Cluster'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dc96f-0e4e-416a-b55c-8e6df5721370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means clustering same with 'effective_literacy_rate_total' instead of total graduates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28007ab7-fdaa-42f9-8606-31d440b97b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#social media advertise kmeans 3\n",
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('data/Social_Network_Ads.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "df['Gender'] = label_encoder.fit_transform(df['Gender'])\n",
    "X = df[['Age', 'EstimatedSalary']]  # We can include 'Age' as well if desired\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.scatterplot(x='Age', y='EstimatedSalary', hue='Cluster', palette='viridis', data=df, s=100)\n",
    "plt.title('K-Means Clustering based on Age and Estimated Salary')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "print(df[['User ID', 'Gender', 'Age', 'EstimatedSalary', 'Cluster']].sort_values(by='Cluster'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f06864-027b-4a91-b92e-7946b00b7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans with 8 initial centroids\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "points = np.array([\n",
    "    [0.1, 0.6],  # P1\n",
    "    [0.15, 0.71],  # P2\n",
    "    [0.08, 0.9],  # P3\n",
    "    [0.16, 0.85],  # P4\n",
    "    [0.2, 0.3],  # P5\n",
    "    [0.25, 0.5],  # P6\n",
    "    [0.24, 0.1],  # P7\n",
    "    [0.3, 0.2]   # P8\n",
    "])\n",
    "m1 = np.array([0.1, 0.6])  # P1\n",
    "m2 = np.array([0.3, 0.2])  # P8\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt(np.sum((p1 - p2) ** 2))\n",
    "    \n",
    "def assign_clusters(points, m1, m2):\n",
    "    cluster_1 = []\n",
    "    cluster_2 = []\n",
    "    \n",
    "    for point in points:\n",
    "        d1 = euclidean_distance(point, m1)\n",
    "        d2 = euclidean_distance(point, m2)\n",
    "        \n",
    "        if d1 < d2:\n",
    "            cluster_1.append(point)\n",
    "        else:\n",
    "            cluster_2.append(point)\n",
    "    \n",
    "    return np.array(cluster_1), np.array(cluster_2)    \n",
    "\n",
    "def calculate_new_centroids(cluster_1, cluster_2):\n",
    "    new_m1 = np.mean(cluster_1, axis=0) if len(cluster_1) > 0 else m1\n",
    "    new_m2 = np.mean(cluster_2, axis=0) if len(cluster_2) > 0 else m2\n",
    "    return new_m1, new_m2\n",
    "\n",
    "def k_means_clustering(points, m1, m2):\n",
    "    # Step 1: Assign points to the nearest centroid\n",
    "    cluster_1, cluster_2 = assign_clusters(points, m1, m2)\n",
    "    \n",
    "    # Step 2: Calculate new centroids\n",
    "    new_m1, new_m2 = calculate_new_centroids(cluster_1, cluster_2)\n",
    "    \n",
    "    return cluster_1, cluster_2, new_m1, new_m2\n",
    "\n",
    "cluster_1, cluster_2, new_m1, new_m2 = k_means_clustering(points, m1, m2)\n",
    "\n",
    "# Question 1: Which cluster does P6 belong to?\n",
    "p6 = np.array([0.25, 0.5])\n",
    "d1 = euclidean_distance(p6, m1)\n",
    "d2 = euclidean_distance(p6, m2)\n",
    "p6_cluster = 1 if d1 < d2 else 2\n",
    "\n",
    "# Question 2: What is the population of the cluster around m2?\n",
    "population_cluster_2 = len(cluster_2)\n",
    "\n",
    "# Question 3: What are the updated values of m1 and m2?\n",
    "updated_m1 = new_m1\n",
    "updated_m2 = new_m2\n",
    "\n",
    "# Output the results\n",
    "print(f\"Cluster 1 (C1):\\n{cluster_1}\")\n",
    "print(f\"Cluster 2 (C2):\\n{cluster_2}\")\n",
    "print(f\"Updated m1: {updated_m1}\")\n",
    "print(f\"Updated m2: {updated_m2}\")\n",
    "\n",
    "# Answer to the questions:\n",
    "print(f\"\\nAnswer to the questions:\")\n",
    "print(f\"1. P6 belongs to Cluster {p6_cluster}\")\n",
    "print(f\"2. The population of Cluster C2 (around m2) is: {population_cluster_2}\")\n",
    "print(f\"3. The updated centroids are m1: {updated_m1} and m2: {updated_m2}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot Cluster 1 in red\n",
    "plt.scatter(cluster_1[:, 0], cluster_1[:, 1], color='red', label='Cluster 1 (C1)', s=100)\n",
    "\n",
    "# Plot Cluster 2 in blue\n",
    "plt.scatter(cluster_2[:, 0], cluster_2[:, 1], color='blue', label='Cluster 2 (C2)', s=100)\n",
    "\n",
    "# Plot the centroids (m1 and m2)\n",
    "plt.scatter(m1[0], m1[1], color='black', marker='X', s=200, label='Centroid m1 (C1)', linewidth=3)\n",
    "plt.scatter(m2[0], m2[1], color='green', marker='X', s=200, label='Centroid m2 (C2)', linewidth=3)\n",
    "\n",
    "# Plot the updated centroids\n",
    "plt.scatter(updated_m1[0], updated_m1[1], color='orange', marker='X', s=200, label='Updated Centroid m1', linewidth=3)\n",
    "plt.scatter(updated_m2[0], updated_m2[1], color='purple', marker='X', s=200, label='Updated Centroid m2', linewidth=3)\n",
    "\n",
    "# Labeling the points and adding legend\n",
    "plt.title(\"K-Means Clustering (1 Iteration)\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc995f-94dd-44c6-b29d-f0e5441ed781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm mobile prices\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('data/test.csv')\n",
    "# Drop the 'id' column as it's just an identifier\n",
    "#df = df.drop(columns=['id'])\n",
    "# Creating a target variable based on 'ram' (you can modify this to suit your needs)\n",
    "def classify_price_range(row):\n",
    "    if row['ram'] < 1000:\n",
    "        return 0  # Low\n",
    "    elif row['ram'] < 2000:\n",
    "        return 1  # Medium\n",
    "    else:\n",
    "        return 2  # High\n",
    "# Apply the classification function to create the 'price_range' column\n",
    "df['price_range'] = df.apply(classify_price_range, axis=1)\n",
    "X = df[['ram', 'battery_power']]\n",
    "y = df['price_range']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "svm = SVC(kernel='linear')  # Using linear kernel for simplicity\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of the SVM model: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0104fd8-bbef-4311-8b06-7ea0716bbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#universal bank SVM\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('data/UniversalBank.csv')\n",
    "X = df.drop(columns=['ID', 'Personal Loan'])  # Features\n",
    "y = df['Personal Loan']  # Target (Personal Loan)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Step 5: Train and test the SVM model (using linear kernel for simplicity)\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm.predict(X_test_scaled)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "log_reg_pred = log_reg.predict(X_test_scaled)\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_pred)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "print(f'Accuracy of SVM: {svm_accuracy * 100:.2f}%')\n",
    "print(f'Accuracy of Logistic Regression: {log_reg_accuracy * 100:.2f}%')\n",
    "print(f'Accuracy of Random Forest: {rf_accuracy * 100:.2f}%')\n",
    "\n",
    "# Step 9: Plot the comparison of model accuracies\n",
    "models = ['SVM', 'Logistic Regression', 'Random Forest']\n",
    "accuracies = [svm_accuracy, log_reg_accuracy, rf_accuracy]\n",
    "plt.bar(models, accuracies, color=['blue', 'green', 'orange'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997fe51-25f3-44e5-9ea7-c0a74b893fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user behaviour svm 3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('data/user_behavior_dataset.csv')\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['Device Model'] = le.fit_transform(df['Device Model'])\n",
    "df['Operating System'] = le.fit_transform(df['Operating System'])\n",
    "df['Gender'] = le.fit_transform(df['Gender'])\n",
    "X = df.drop(columns=['User ID', 'User Behavior Class'])\n",
    "y = df['User Behavior Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm.predict(X_test_scaled)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "print(f'Accuracy of SVM model: {svm_accuracy * 100:.2f}%')\n",
    "plt.bar(['SVM'], [svm_accuracy], color='blue')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('SVM Model Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d609986-1fda-4a70-93b9-de76a70fc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user behavior svm 4\n",
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "df = pd.read_csv('data/bank_transactions_data_2.csv')\n",
    "df.dropna(inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "# Columns that need encoding\n",
    "categorical_columns = ['TransactionType', 'Location', 'DeviceID', 'MerchantID', 'Channel', 'CustomerOccupation']\n",
    "for col in categorical_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "features = ['TransactionAmount', 'CustomerAge', 'TransactionDuration', 'LoginAttempts', 'AccountBalance']\n",
    "X = df[features]\n",
    "# Target variable: let's predict 'TransactionType' (Debit=0, Credit=1)\n",
    "y = df['TransactionType']\n",
    "#Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Step 4: Standardize the features (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "print(\"SVM with Linear Kernel Accuracy: \", accuracy_linear)\n",
    "print(\"Classification Report for Linear Kernel:\")\n",
    "print(classification_report(y_test, y_pred_linear))\n",
    "svm_poly = SVC(kernel='poly', degree=3)\n",
    "svm_poly.fit(X_train_scaled, y_train)\n",
    "# Make predictions for Polynomial Kernel\n",
    "y_pred_poly = svm_poly.predict(X_test_scaled)\n",
    "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
    "print(\"SVM with Polynomial Kernel Accuracy: \", accuracy_poly)\n",
    "print(\"Classification Report for Polynomial Kernel:\")\n",
    "print(classification_report(y_test, y_pred_poly))\n",
    "if accuracy_linear > accuracy_poly:\n",
    "    print(\"The Linear Kernel model performs better.\")\n",
    "else:\n",
    "    print(\"The Polynomial Kernel model performs better.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebfa13-bbc5-44d2-89c7-dfea95d028f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apriori market basket analysis\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv(\"data/Order1.csv\")\n",
    "# Step 2: Data preprocessing\n",
    "# Renaming columns for clarity (optional)\n",
    "data.rename(columns={\"Member_number\": \"TransactionID\", \"itemDescription\": \"Item\"}, inplace=True)\n",
    "# Step 3: Group transactions by TransactionID\n",
    "# Each transaction will be a list of items\n",
    "transactions = data.groupby(\"TransactionID\")[\"Item\"].apply(list)\n",
    "# Step 4: Create a one-hot encoded DataFrame for Apriori\n",
    "# Flatten the transactions to create a binary matrix\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_data, columns=te.columns_)\n",
    "# Step 5: Apply the Apriori algorithm\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.09, use_colnames=True)\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "# Step 6: Generate association rules\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "# Generate association rules from frequent itemsets\n",
    "rules = association_rules(frequent_itemsets,num_itemsets=2, metric=\"lift\", min_threshold=1.0)\n",
    "# Display association rules\n",
    "print(\"Association Rules:\")\n",
    "print(rules)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='support', y='antecedents', data=rules)\n",
    "plt.title('Association Rules Support')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Antecedents')\n",
    "plt.show()\n",
    "# Visualization 2: Plot the lift of the association rules\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='lift', y='antecedents', data=rules)\n",
    "plt.title('Association Rules Lift')\n",
    "plt.xlabel('Lift')\n",
    "plt.ylabel('Antecedents')\n",
    "plt.show()\n",
    "# Visualization 3: Scatter plot of support vs. lift for the association rules\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='support', y='lift', data=rules)\n",
    "plt.title('Support vs. Lift for Association Rules')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Lift')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303640d1-bc10-41b0-ab29-96b461176427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apriori order 2\n",
    "#data shaping\n",
    "transactions = []\n",
    "for i in range(data.shape[0]):  # Iterate through rows\n",
    "    transaction = [str(data.iloc[i, j]) for j in range(data.shape[1]) if pd.notnull(data.iloc[i, j])]\n",
    "    transactions.append(transaction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
